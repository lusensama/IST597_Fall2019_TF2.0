{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define paramaters for the model\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "n_epochs = 2\n",
    "n_train = None\n",
    "n_test = None\n",
    "n_classes = 10\n",
    "n_channel = 1\n",
    "pixel_depth = 255\n",
    "image_size = 28\n",
    "seed = None\n",
    "val_size = 0.2\n",
    "\n",
    "# Step 1: Read in data\n",
    "fmnist_folder = 'None'\n",
    "\n",
    "\n",
    "# Create dataset load function [Refer fashion mnist github page for util function]\n",
    "# Create train,validation,test split\n",
    "# train, val, test = utils.read_fmnist(fmnist_folder, flatten=True)\n",
    "def load_mnist(path, kind='train', val_size=0.0):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "    # if kind=='train':\n",
    "    #     return train_test_split(images, labels, test_size=val_size, random_state=13)\n",
    "    # else:\n",
    "    # img = images.reshape([-1, 28, 28, 1])\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "# Step 2: Create datasets and iterator\n",
    "# create training Dataset and batch it\n",
    "fmnist_folder = './data/'\n",
    "train_dataset, train_labelset = load_mnist('./data/')\n",
    "test_dataset, test_labelset = load_mnist('./data/', kind='t10k')\n",
    "np.random.seed(seed)\n",
    "train_index = np.random.choice(len(train_dataset), round(len(train_dataset) * (1.0 - val_size)), replace=False)\n",
    "val_index = np.array(list(set(range(len(train_dataset))) - set(train_index)))\n",
    "train_data = train_dataset[train_index]\n",
    "train_label = train_labelset[train_index]\n",
    "val_data = train_dataset[val_index]\n",
    "val_label = train_labelset[val_index]\n",
    "\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_label)).batch(batch_size)\n",
    "# # val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_label))\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_label)).batch(batch_size)\n",
    "\n",
    "\n",
    "# create one iterator and initialize it with different datasets\n",
    "# iterator = tf.data.Iterator.from_structure(train_data.output_types,\n",
    "#                                            train_data.output_shapes)\n",
    "# img, label = iterator.get_next()\n",
    "\n",
    "# train_init = iterator.make_initializer(train_data)\t# initializer for train_data\n",
    "# test_init = iterator.make_initializer(test_data)\t# initializer for train_data\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random.normal([batch_size, 28 * 28]), dtype=tf.float32)\n",
    "b = tf.Variable(tf.random.normal([batch_size, 1], dtype=tf.float32))\n",
    "\n",
    "batch_index = np.random.choice(len(train_data), size=batch_size)\n",
    "batch_train_X = train_data[batch_index]\n",
    "batch_train_y = np.matrix(train_label[batch_index]).T\n",
    "\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "# iterator = train_dataset.__iter__()\n",
    "# train_img, train_lbl = iterator.next()\n",
    "# itert = test_dataset.__iter__()\n",
    "# test_img, test_lbl = iter.next()\n",
    "\n",
    "def logi(data, W, b):\n",
    "    return data * W + b\n",
    "\n",
    "\n",
    "logits = logi(batch_train_X, w, b)\n",
    "\n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "def loss_func(logit, y):\n",
    "    return tf.reduce_mean(tf.compat.v2.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "\n",
    "\n",
    "loss = loss_func(logits, batch_train_y)\n",
    "\n",
    "# Step 6: define optimizer\n",
    "# using Adam Optimizer with pre-defined learning rate to minimize loss\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "# Step 7: calculate accuracy with test set\n",
    "preds = tf.nn.softmax(logits)\n",
    "correct_preds = tf.equal(tf.argmax(preds, -1), tf.argmax(val_label, -1))\n",
    "accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\n",
    "print(accuracy.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    # Generate random batch index\n",
    "    batch_index = np.random.choice(len(train_data), size=batch_size)\n",
    "    batch_train_X = train_data[batch_index]\n",
    "    batch_train_y = np.matrix(train_label[batch_index]).T\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = logi(batch_train_X, w, b)\n",
    "        loss = loss_func(logits, batch_train_y)\n",
    "    dW, db = tape.gradient(loss, [w,b])\n",
    "\n",
    "    # if tf.equal(loss, prevloss):\n",
    "    #     learning_rate /= 2\n",
    "    w.assign_sub(dW * learning_rate)\n",
    "    b.assign_sub(db * learning_rate)\n",
    "    \n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct_preds = tf.equal(tf.argmax(preds, -1), tf.argmax(val_label, -1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\n",
    "    print(accuracy.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=3398108, shape=(32,), dtype=bool, numpy=\n",
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_preds = tf.equal(tf.argmax(preds, axis=-1), tf.argmax(test_lbl, axis=-1))\n",
    "correct_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=3398102, shape=(32, 10), dtype=float32, numpy=\n",
       "array([[7.07752336e+17, 7.07713578e+17, 7.07625480e+17, 7.07765805e+17,\n",
       "        7.07771715e+17, 7.07703683e+17, 7.07693238e+17, 7.07755085e+17,\n",
       "        7.07735981e+17, 7.07814390e+17],\n",
       "       [1.38965969e+17, 1.38958358e+17, 1.38941067e+17, 1.38968632e+17,\n",
       "        1.38969783e+17, 1.38956400e+17, 1.38954355e+17, 1.38966519e+17,\n",
       "        1.38962748e+17, 1.38978149e+17],\n",
       "       [1.97826881e+17, 1.97816023e+17, 1.97791422e+17, 1.97830626e+17,\n",
       "        1.97832310e+17, 1.97813274e+17, 1.97810354e+17, 1.97827637e+17,\n",
       "        1.97822311e+17, 1.97844233e+17],\n",
       "       [4.07238629e+17, 4.07216329e+17, 4.07165648e+17, 4.07246359e+17,\n",
       "        4.07249761e+17, 4.07210625e+17, 4.07204578e+17, 4.07240209e+17,\n",
       "        4.07229180e+17, 4.07274294e+17],\n",
       "       [4.07498491e+17, 4.07476192e+17, 4.07425442e+17, 4.07506291e+17,\n",
       "        4.07509624e+17, 4.07470454e+17, 4.07464441e+17, 4.07500037e+17,\n",
       "        4.07489008e+17, 4.07534225e+17],\n",
       "       [4.67844912e+17, 4.67819314e+17, 4.67761040e+17, 4.67853811e+17,\n",
       "        4.67857660e+17, 4.67812786e+17, 4.67805811e+17, 4.67846699e+17,\n",
       "        4.67834089e+17, 4.67885903e+17],\n",
       "       [5.04761943e+17, 5.04734317e+17, 5.04671508e+17, 5.04771563e+17,\n",
       "        5.04775755e+17, 5.04727239e+17, 5.04719749e+17, 5.04763901e+17,\n",
       "        5.04750260e+17, 5.04806267e+17],\n",
       "       [9.45301274e+17, 9.45249528e+17, 9.45131811e+17, 9.45319416e+17,\n",
       "        9.45327112e+17, 9.45236265e+17, 9.45222246e+17, 9.45304985e+17,\n",
       "        9.45279421e+17, 9.45384149e+17],\n",
       "       [7.70018230e+17, 7.69976036e+17, 7.69880241e+17, 7.70032935e+17,\n",
       "        7.70039258e+17, 7.69965316e+17, 7.69953908e+17, 7.70021184e+17,\n",
       "        7.70000362e+17, 7.70085712e+17],\n",
       "       [4.17880149e+17, 4.17857231e+17, 4.17805279e+17, 4.17888120e+17,\n",
       "        4.17891591e+17, 4.17851390e+17, 4.17845205e+17, 4.17881764e+17,\n",
       "        4.17870459e+17, 4.17916776e+17],\n",
       "       [6.68172804e+17, 6.68136314e+17, 6.68053026e+17, 6.68185586e+17,\n",
       "        6.68191152e+17, 6.68126899e+17, 6.68117004e+17, 6.68175415e+17,\n",
       "        6.68157411e+17, 6.68231422e+17],\n",
       "       [3.11856064e+17, 3.11838987e+17, 3.11800195e+17, 3.11862042e+17,\n",
       "        3.11864585e+17, 3.11834623e+17, 3.11830019e+17, 3.11857266e+17,\n",
       "        3.11848848e+17, 3.11883414e+17],\n",
       "       [9.00182402e+17, 9.00133130e+17, 9.00021048e+17, 9.00199582e+17,\n",
       "        9.00207072e+17, 9.00120554e+17, 9.00107154e+17, 9.00185975e+17,\n",
       "        9.00161511e+17, 9.00261360e+17],\n",
       "       [6.52413091e+17, 6.52377357e+17, 6.52296131e+17, 6.52425530e+17,\n",
       "        6.52430890e+17, 6.52368218e+17, 6.52358528e+17, 6.52415565e+17,\n",
       "        6.52397973e+17, 6.52470266e+17],\n",
       "       [3.69282147e+17, 3.69261909e+17, 3.69215970e+17, 3.69289156e+17,\n",
       "        3.69292215e+17, 3.69256755e+17, 3.69251258e+17, 3.69283556e+17,\n",
       "        3.69273592e+17, 3.69314514e+17],\n",
       "       [6.67573982e+17, 6.67537424e+17, 6.67454273e+17, 6.67586764e+17,\n",
       "        6.67592193e+17, 6.67528078e+17, 6.67518182e+17, 6.67576525e+17,\n",
       "        6.67558452e+17, 6.67632531e+17],\n",
       "       [6.23837540e+17, 6.23803387e+17, 6.23725734e+17, 6.23849429e+17,\n",
       "        6.23854651e+17, 6.23794728e+17, 6.23785451e+17, 6.23839945e+17,\n",
       "        6.23823178e+17, 6.23892310e+17],\n",
       "       [7.36645440e+17, 7.36605171e+17, 7.36513361e+17, 7.36659596e+17,\n",
       "        7.36665575e+17, 7.36594794e+17, 7.36583868e+17, 7.36648326e+17,\n",
       "        7.36628398e+17, 7.36710037e+17],\n",
       "       [5.60450695e+17, 5.60419977e+17, 5.60350227e+17, 5.60461346e+17,\n",
       "        5.60465950e+17, 5.60412143e+17, 5.60403759e+17, 5.60452825e+17,\n",
       "        5.60437672e+17, 5.60499829e+17],\n",
       "       [5.51776201e+17, 5.51745999e+17, 5.51677279e+17, 5.51786749e+17,\n",
       "        5.51791319e+17, 5.51738302e+17, 5.51730056e+17, 5.51778331e+17,\n",
       "        5.51763453e+17, 5.51824579e+17],\n",
       "       [4.05861009e+17, 4.05838778e+17, 4.05788304e+17, 4.05868774e+17,\n",
       "        4.05872142e+17, 4.05833143e+17, 4.05827062e+17, 4.05862555e+17,\n",
       "        4.05851595e+17, 4.05896606e+17],\n",
       "       [1.04234919e+18, 1.04229201e+18, 1.04216234e+18, 1.04236905e+18,\n",
       "        1.04237764e+18, 1.04227751e+18, 1.04226205e+18, 1.04235317e+18,\n",
       "        1.04232500e+18, 1.04244051e+18],\n",
       "       [3.30730727e+17, 3.30712619e+17, 3.30671456e+17, 3.30737083e+17,\n",
       "        3.30739798e+17, 3.30708015e+17, 3.30703067e+17, 3.30732067e+17,\n",
       "        3.30723065e+17, 3.30759761e+17],\n",
       "       [7.45503450e+17, 7.45462699e+17, 7.45369859e+17, 7.45517674e+17,\n",
       "        7.45523859e+17, 7.45452254e+17, 7.45441121e+17, 7.45506336e+17,\n",
       "        7.45486201e+17, 7.45568802e+17],\n",
       "       [6.18479070e+17, 6.18445191e+17, 6.18368157e+17, 6.18490821e+17,\n",
       "        6.18495975e+17, 6.18436533e+17, 6.18427324e+17, 6.18481475e+17,\n",
       "        6.18464708e+17, 6.18533290e+17],\n",
       "       [4.57601415e+17, 4.57576332e+17, 4.57519364e+17, 4.57610108e+17,\n",
       "        4.57613888e+17, 4.57569942e+17, 4.57563173e+17, 4.57603202e+17,\n",
       "        4.57590764e+17, 4.57641513e+17],\n",
       "       [5.86614261e+17, 5.86582169e+17, 5.86509052e+17, 5.86625462e+17,\n",
       "        5.86630342e+17, 5.86573923e+17, 5.86565264e+17, 5.86616529e+17,\n",
       "        5.86600655e+17, 5.86665732e+17],\n",
       "       [3.92887150e+17, 3.92865606e+17, 3.92816712e+17, 3.92894606e+17,\n",
       "        3.92897870e+17, 3.92860143e+17, 3.92854268e+17, 3.92888662e+17,\n",
       "        3.92878045e+17, 3.92921613e+17],\n",
       "       [5.29652068e+17, 5.29623069e+17, 5.29557167e+17, 5.29662239e+17,\n",
       "        5.29666568e+17, 5.29615647e+17, 5.29607813e+17, 5.29654130e+17,\n",
       "        5.29639768e+17, 5.29698557e+17],\n",
       "       [5.28452914e+17, 5.28423983e+17, 5.28358184e+17, 5.28462981e+17,\n",
       "        5.28467345e+17, 5.28416595e+17, 5.28408727e+17, 5.28454941e+17,\n",
       "        5.28440647e+17, 5.28499299e+17],\n",
       "       [1.90075771e+17, 1.90065360e+17, 1.90041686e+17, 1.90079378e+17,\n",
       "        1.90080959e+17, 1.90062679e+17, 1.90059879e+17, 1.90076492e+17,\n",
       "        1.90071338e+17, 1.90092401e+17],\n",
       "       [8.85861228e+16, 8.85812781e+16, 8.85702486e+16, 8.85878151e+16,\n",
       "        8.85885452e+16, 8.85800412e+16, 8.85787269e+16, 8.85864664e+16,\n",
       "        8.85840784e+16, 8.85939053e+16]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=566550, shape=(32,), dtype=bool, numpy=\n",
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False])>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
